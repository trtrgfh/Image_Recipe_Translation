{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "663cef75-121b-4a2a-adb8-7ae2caa75760",
   "metadata": {
    "id": "663cef75-121b-4a2a-adb8-7ae2caa75760"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vit_b_32\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a6502b-8545-4947-a080-8cda4f14263b",
   "metadata": {
    "id": "54a6502b-8545-4947-a080-8cda4f14263b",
    "outputId": "942c8799-4948-4609-abfb-d4016c0ed33c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1792b7ab-77ed-4b0b-82b7-09bd01867cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topicModel = 'LDA' # LDA or NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92d6046a-6abc-4746-95af-6b5ca86d2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./data/processed/{topicModel}_train.pkl', 'rb') as f:\n",
    "    train_dataset = pickle.load(f) \n",
    "with open(f'./data/processed/{topicModel}_val.pkl', 'rb') as f:\n",
    "    val_dataset = pickle.load(f) \n",
    "with open(f'./data/processed/{topicModel}_test.pkl', 'rb') as f:\n",
    "    test_dataset = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdcadfd0-fe2c-42c6-acb6-e2f998a2c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_array, transform=None):\n",
    "        self.data_array = data_array\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_array)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path, label, image = self.data_array[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if isinstance(label, str):\n",
    "            label = int(label)\n",
    "        \n",
    "        return image_path, label, image\n",
    "    \n",
    "train_dataset = CustomDataset(train_dataset, transform=None)\n",
    "val_dataset = CustomDataset(val_dataset, transform=None)\n",
    "test_dataset = CustomDataset(test_dataset, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "364f0f37-8316-4725-91d6-1f4e0339c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batchSize, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff41aa0a-ca52-4bc6-befd-187691f1e7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (conv_proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      (encoder_layer_0): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_1): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_2): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_3): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_4): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_5): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_6): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_7): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_8): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_9): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_10): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_11): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (heads): Sequential(\n",
      "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = vit_b_32(weights='IMAGENET1K_V1').to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21a2730a-6145-43b8-96d5-be9edda51360",
   "metadata": {
    "id": "21a2730a-6145-43b8-96d5-be9edda51360",
    "outputId": "370ab5d0-b644-4f34-e3d3-74991e17834b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yehao\\anaconda3\\envs\\image_recipe\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1221: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 1024\n",
      "finished 2048\n",
      "finished 3072\n",
      "finished 4096\n",
      "finished 5120\n",
      "finished 6144\n",
      "finished 7168\n",
      "finished 8192\n",
      "finished 9216\n",
      "finished 10240\n",
      "finished 11264\n",
      "finished 12288\n",
      "finished 13312\n",
      "finished 14336\n",
      "finished 15360\n",
      "finished 16384\n",
      "finished 17408\n",
      "finished 18432\n",
      "finished 19456\n",
      "finished 20480\n",
      "finished 21504\n",
      "finished 22528\n",
      "finished 23552\n",
      "finished 24576\n",
      "finished 25600\n",
      "finished 26624\n",
      "finished 27648\n",
      "finished 28672\n",
      "finished 29696\n",
      "finished 30720\n",
      "finished 31744\n",
      "finished 32768\n",
      "finished 33792\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "labels_train = []\n",
    "imagePath_train = []\n",
    "def capture_input(module, input, output):\n",
    "    # input[0] contains the input tensor to the FC layer for the entire batch\n",
    "    for i in range(input[0].shape[0]):\n",
    "        features.append(input[0][i].detach().cpu().numpy())\n",
    "\n",
    "# Define the hook\n",
    "handle = model.heads.head.register_forward_hook(capture_input)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (imagePaths, labels, inputs) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        _ = model(inputs)\n",
    "        labels_train.extend(labels.detach().cpu().numpy().tolist())\n",
    "        imagePath_train.extend(list(imagePaths))\n",
    "        print('finished ' + str((i+1)*batchSize))\n",
    "\n",
    "features_train = np.array(features)\n",
    "np.savez('./data/interim/model1_features_train_' + topicModel + '.npz', features_train=features_train, labels_train=labels_train, imagePath_train=imagePath_train)\n",
    "# # Remove the hook\n",
    "# handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "808011b5-4882-4450-a3aa-f6a699014783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 1024\n",
      "finished 2048\n",
      "finished 3072\n",
      "finished 4096\n",
      "finished 5120\n",
      "finished 6144\n",
      "finished 7168\n",
      "finished 8192\n",
      "finished 9216\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "labels_val = []\n",
    "imagePath_val = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (imagePaths, labels, inputs) in enumerate(val_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        _ = model(inputs)\n",
    "        labels_val.extend(labels.detach().cpu().numpy().tolist())\n",
    "        imagePath_val.extend(list(imagePaths))\n",
    "        print('finished ' + str((i+1)*batchSize))\n",
    "\n",
    "features_val = np.array(features)\n",
    "np.savez('./data/interim/model1_features_val_' + topicModel + '.npz', features_val=features_val, labels_test=labels_val,imagePath_test=imagePath_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d65d5e9-561f-49af-ba23-61407071f160",
   "metadata": {
    "id": "0d65d5e9-561f-49af-ba23-61407071f160",
    "outputId": "397d87c4-3ae3-4749-a594-fff40ae4c261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 1024\n",
      "finished 2048\n",
      "finished 3072\n",
      "finished 4096\n",
      "finished 5120\n",
      "finished 6144\n",
      "finished 7168\n",
      "finished 8192\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "labels_test = []\n",
    "imagePath_test = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (imagePaths, labels, inputs) in enumerate(test_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        _ = model(inputs)\n",
    "        labels_test.extend(labels.detach().cpu().numpy().tolist())\n",
    "        imagePath_test.extend(list(imagePaths))\n",
    "        print('finished ' + str((i+1)*batchSize))\n",
    "\n",
    "features_test = np.array(features)\n",
    "np.savez('./data/interim/model1_features_test_' + topicModel + '.npz', features_test=features_test, labels_test=labels_test,imagePath_test=imagePath_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b24af37-4c5e-4bc2-8d9b-88b9886748f3",
   "metadata": {
    "id": "4b24af37-4c5e-4bc2-8d9b-88b9886748f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33687, 768), (8422, 768), (7431, 768))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape, features_val.shape, features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2b20c-7dff-4c1f-9e7f-0afecf754dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "image_recipe",
   "language": "python",
   "name": "image_recipe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
